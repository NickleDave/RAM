{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ram.dataset.train('~/data/mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "ram_model = ram.RAM(batch_size=batch_size)\n",
    "optimizer = tf.train.MomentumOptimizer(momentum=0.9, learning_rate=0.001)\n",
    "batch = train.batch(batch_size)\n",
    "img, lbl = next(iter(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "out_t_minus_1 = ram_model.reset()\n",
    "\n",
    "locs = []\n",
    "mus = []\n",
    "log_pis = []\n",
    "baselines = []\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    for t in range(ram_model.glimpses):\n",
    "        out = ram_model.step(img, out_t_minus_1.l_t, out_t_minus_1.h_t)\n",
    "\n",
    "        locs.append(out.l_t)\n",
    "        mus.append(out.mu)\n",
    "        baselines.append(out.b_t)\n",
    "\n",
    "        # determine probability of choosing location l_t, given\n",
    "        # distribution parameterized by mu (output of location network)\n",
    "        # and the constant standard deviation specified as a parameter.\n",
    "        # Assume both dimensions are independent\n",
    "        # 1. we get log probability from pdf for each dimension\n",
    "        # 2. we want the joint distribution which is the product of the pdfs\n",
    "        # 3. so we sum the log prob, since log(p(x) * p(y)) = log(p(x)) + log(p(y))\n",
    "        mu_distrib = tf.distributions.Normal(loc=out.mu,\n",
    "                                             scale=ram_model.loc_std)\n",
    "        log_pi = mu_distrib.log_prob(value=out.l_t)\n",
    "        log_pi = tf.reduce_sum(log_pi, axis=1)\n",
    "        log_pis.append(log_pi)\n",
    "\n",
    "        out_t_minus_1 = out\n",
    "\n",
    "    # convert lists to tensors, reshape to (batch size x number of glimpses)\n",
    "    # for calculations below\n",
    "    baselines = tf.stack(baselines)\n",
    "    baselines = tf.squeeze(baselines)\n",
    "    baselines = tf.transpose(baselines, perm=[1, 0])\n",
    "\n",
    "    log_pis = tf.stack(log_pis)\n",
    "    log_pis = tf.squeeze(log_pis)\n",
    "    log_pis = tf.transpose(log_pis, perm=[1, 0])\n",
    "\n",
    "    # repeat column vector n times where n = glimpses\n",
    "    # calculate reward\n",
    "    predicted = tf.argmax(out.a_t, axis=1, output_type=tf.int32)  # a_t = predictions from last time step\n",
    "    R = tf.equal(predicted, lbl)\n",
    "    R = tf.cast(R, dtype=tf.float32)\n",
    "    # reshape reward to (batch size x number of glimpses)\n",
    "    R = tf.expand_dims(R, axis=1)  # add axis\n",
    "    R = tf.tile(R, tf.constant([1, ram_model.glimpses]))\n",
    "\n",
    "    # compute losses for differentiable modules\n",
    "    loss_action = tf.losses.softmax_cross_entropy(tf.one_hot(lbl, depth=ram_model.num_classes), out.a_t)\n",
    "    loss_baseline = tf.losses.mean_squared_error(baselines, R)\n",
    "\n",
    "    # compute loss for REINFORCE algorithm\n",
    "    # summed over timesteps and averaged across batch\n",
    "    adjusted_reward = R - baselines\n",
    "    loss_reinforce = tf.reduce_sum((-log_pis * adjusted_reward), axis=1)\n",
    "    loss_reinforce = tf.reduce_mean(loss_reinforce)\n",
    "\n",
    "    # sum up into hybrid loss\n",
    "    hybrid_loss = loss_action + loss_baseline + loss_reinforce\n",
    "\n",
    "# apply reinforce loss **only** to location network and baseline network\n",
    "lt_bt_params = [var for net in [ram_model.location_network,\n",
    "                          ram_model.baseline]\n",
    "          for var in net.variables]\n",
    "reinforce_grads = tape.gradient(loss_reinforce, lt_bt_params)\n",
    "optimizer.apply_gradients(zip(reinforce_grads, lt_bt_params),\n",
    "                          global_step=tf.train.get_or_create_global_step())\n",
    "# apply hybrid loss to glimpse network, core network, and action network\n",
    "params = [var for net in [ram_model.glimpse_network,\n",
    "                          ram_model.action_network,\n",
    "                          ram_model.core_network]\n",
    "          for var in net.variables]\n",
    "hybrid_grads = tape.gradient(hybrid_loss, params)\n",
    "optimizer.apply_gradients(zip(hybrid_grads, params),\n",
    "                          global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
