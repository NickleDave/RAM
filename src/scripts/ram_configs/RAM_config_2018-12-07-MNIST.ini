[model]
g_w = 8
k = 3
s = 2
hg_size = 128
hl_size = 128
g_size = 256
hidden_size = 256
glimpses = 6
num_classes = 10
loc_std = 0.1

[train]
batch_size = 200
learning_rate = 1e-3
epochs = 200
optimizer = momentum
momentum = 0.9
checkpoint_dir = /home/art/Documents/repos/coding/L2M/ram_output/2018-12-07-MNIST/checkpoints
checkpoint_prefix = ckpt
restore = False
save_examples_every = 25
examples_dir = /home/art/Documents/repos/coding/L2M/ram_output/2018-12-07-MNIST/examples
num_examples_to_save = 9
save_loss = True
loss_dir = /home/art/Documents/repos/coding/L2M/ram_output/2018-12-07-MNIST/loss