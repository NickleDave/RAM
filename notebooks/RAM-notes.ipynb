{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Attention Model\n",
    "considers attention as a sequential decision process of a goal-directed agent interacting with a visual environement\n",
    "\n",
    "### model\n",
    "Agent built around a recurrent neural network\n",
    "At each time step, agent processes sensor data, integrates information over time, and chooses how to act and how to deploy its sensor at the next time step.\n",
    "\n",
    "#### sensor\n",
    "At each step $t$ the agent receives a (partial) observation of the environment in the form of an image $x_t$.  \n",
    "The agent does not have full access to the image; rather, it extracts information from $x_t$ via its bandwidth limited sensor $\\rho$, i.e. by focusing the sensor on some region of interest.\n",
    "\n",
    "We assume the sensor extracts a retina-like representation $\\rho (x_t, l_{t-1})$ around location $l_{t-1}$ from image $x_t$. The sensor encode $l$ such that resolution is lower the farther away we are from the center of $l$, resulting in a vector of much lower dimensionality than the original image $x_t$. We refer to this low-resolution representation as a *glimpse*. The glimpse sensor is used inside what we call the *glimpse network $f_g$* to produce the glimpse feature vector $g_t = f_g(x_t, l_{t-1}; \\theta_g)$, where $\\theta_g = \\{\\theta_g^0, \\theta_g^1, \\theta_g^2\\}$\n",
    "\n",
    "#### internal state\n",
    "The internal state is formed by hidden units $h_t$ of recurrent neural network (RNN), updated over time by the *core network*: $h_t = f_h(h_t-1, g_t; \\theta_t)$ where $g_t$ is the glimpse feature vector that is the only external input to the network.\n",
    "\n",
    "#### actions\n",
    "The agent performs two actions on each time step:\n",
    "1. it deploys its sensor via the sensor control $l_t$\n",
    "2. it takes an action on the environment $a_t$ which might cause the environment to change state. The nature of environment actions depend on the task.  \n",
    "\n",
    "Location actions are chosen stochastically from a distribution parameterized by the location network $f_l(h_t;\\theta_l)$ at time $t$: $l_t \\sim p(\\cdot | f_l(h_t;\\theta_l))$. Environment actions similarly are drawn from a distribution conditioned on a second network output $a_t \\sim p(\\cdot | f_a(h_t; \\theta_a))$.\n",
    "\n",
    "#### reward\n",
    "After executing two actions on one time step, the agent receives a new visual input $x_{t+1}$ and a reward signal $r_{t+1}$. The goal of the agent is to maximize the sum of the reward signal, which is usually very sparse and delayed: $R = \\sum_{t=1}^T r_t$. E.g., for object recognition, $r_T = 1$ if the object is classified correctly after $T$ steps and 0 otherwise.\n",
    "\n",
    "\n",
    "The above is a case of a Partially Observable Markov Decision Process (POMDP). The true state of the environment is not observed, so the agent learns a stocahstic policy $\\pi((l_t, a_t) | s_{1:t};\\theta)$ with parameters $\\theta$ that, at each time step $t$, map the history of past interactions with the environment $s_{1:t} = x_1, l_1, a_1, ... , x_{t-1}, l_{t-1}, a_{t-1}, x_t$ to a distribution over actions for the current time step, subject to the constraint of the sensor. The policy $\\pi$ is defined by the RNN, and the history $s_t$ is summarized in the tate of the hidden units $h_t$ in the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
