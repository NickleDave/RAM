{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests of speed of some `tf.data` pipelines on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/art/anaconda3/envs/RAM-Rescience-env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some data\n",
    "We start with Numpy arrays and ask how steps later in the pipeline affect speed, neglecting question of whether we need `tf.records`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_train():\n",
    "    images = np.load('/home/art/Documents/data/mnist/raw/train_images.npy')\n",
    "    labels = np.load('/home/art/Documents/data/mnist/raw/train_labels.npy')\n",
    "    sample_inds = np.load('/home/art/Documents/data/mnist/raw/train_indices.npy')\n",
    "    return images, labels, sample_inds\n",
    "\n",
    "# # small dataset so we don't have to sit through all 50k images of MNIST\n",
    "# images = images[:1000]\n",
    "# labels = labels[:1000]\n",
    "# sample_inds = sample_inds[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a model to test with\n",
    "We create a very simple Keras model and logic for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = np.prod(images.shape[1:])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape(\n",
    "        target_shape=(target_shape,)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu, input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline 1\n",
    "What I've been using for RAM model, adapted from tf.contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images):\n",
    "    \"\"\"\n",
    "    Normalize images to [0.0,1.0]\n",
    "    \"\"\"\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255.\n",
    "    return images\n",
    "\n",
    "def pipeline_one(images, labels, sample_inds):\n",
    "    images = normalize(images)\n",
    "\n",
    "    def gen():\n",
    "        for image, label, batch_sample_inds in zip(images, labels, sample_inds):\n",
    "            yield image, label, batch_sample_inds\n",
    "\n",
    "    data = tf.data.Dataset.from_generator(gen, (tf.float32, tf.int32, tf.int32),\n",
    "                                          ((28, 28, 1), [], []))\n",
    "    data = data.shuffle(buffer_size=sample_inds.shape[-1], reshuffle_each_iteration=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "images, labels, sample_inds = load_mnist_train()\n",
    "data = pipeline_one(images, labels, sample_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6 s ± 444 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "# in eager mode, no need to create iterator from dataset or call get_next()\n",
    "for img, lbl, batch_train_inds in data.batch(batch_size=32):\n",
    "    # Optimize the model\n",
    "    lbl = tf.one_hot(indices=lbl, depth=10)\n",
    "    loss_value, grads = grad(model, img, lbl)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
    "                              global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_two(images, labels, sample_inds):\n",
    "    data = tf.data.Dataset.from_tensor_slices((images, labels, sample_inds))\n",
    "    data = data.map(map_func=lambda img, lbl, inds: tuple([normalize(img), lbl, inds]), \n",
    "                    num_parallel_calls = 4)\n",
    "    data.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=sample_inds.shape[-1]))\n",
    "    return data\n",
    "\n",
    "images, labels, sample_inds = load_mnist_train()\n",
    "data = pipeline_two(images, labels, sample_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 s ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "for img, lbl, batch_train_inds in data.batch(batch_size=32):\n",
    "    # Optimize the model\n",
    "    lbl = tf.one_hot(indices=lbl, depth=10)\n",
    "    loss_value, grads = grad(model, img, lbl)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
    "                              global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
